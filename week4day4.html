<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Week 4 - Day 4</title>
  <style>
    body { font-family: Arial, sans-serif; background-color: #fdfdfd; color: #333; line-height: 1.6; max-width: 900px; margin: auto; padding: 40px 20px; }
    h1, h2, h3 { color: #2c3e50; text-align: left; }
    pre { background: #f4f4f4; padding: 10px; border-left: 4px solid #3498db; overflow-x: auto; }
    .concept-box { background-color: #e0f7fa; border-left: 5px solid #00796b; padding: 10px 20px; margin: 20px 0; }
    p { margin-bottom: 10px; }
  </style>
</head>
<body>
  <h1>Week 4 - Day 4</h1>
  <h2>Data Preprocessing & Feature Engineering in R</h2>

  <div class="concept-box">
    <strong>Concepts Covered:</strong><br>
    Handling missing values, scaling, normalization, encoding categorical variables, feature creation, outlier detection, practical tips
  </div>

  <h3>1. Handling Missing Values</h3>
  <p>Missing values are common in real-world datasets due to incomplete data entry, sensor errors, or survey non-responses. Handling them correctly is crucial because they can bias analysis and degrade model performance. Strategies include removing missing values, imputing with mean/median/mode, or using predictive models. Imputation should consider data distribution to avoid distorting variance.</p>
  <p><strong>Example:</strong> In a sales dataset, missing revenue values can be imputed using the mean revenue for that product category to maintain dataset integrity.</p>
  <pre><code>data <- data.frame(x = c(1, 2, NA, 4, 5))
is.na(data$x)
data$x[is.na(data$x)] <- mean(data$x, na.rm = TRUE)</code></pre>

  <h3>2. Scaling & Normalization</h3>
  <p>Scaling and normalization adjust the range of numeric variables to make them comparable. Min-Max normalization rescales values to [0,1], while Z-score standardization centers data around zero with unit variance. These methods improve the performance of distance-based algorithms like KNN or clustering and help gradient-based models converge faster.</p>
  <p><strong>Example:</strong> Normalizing sensor readings ensures that temperature and humidity contribute equally to a predictive model.</p>
  <pre><code># Min-Max Normalization
normalize <- function(x) (x - min(x)) / (max(x) - min(x))
normalize(c(2, 4, 6, 8, 10))

# Z-score Standardization
scale(c(2, 4, 6, 8, 10))</code></pre>

  <h3>3. Encoding Categorical Variables</h3>
  <p>Machine learning models require numerical inputs, so categorical variables must be converted. Factor encoding maps categories to integers, while one-hot encoding creates separate binary columns. Proper encoding preserves information without introducing bias. Consider ordinal vs nominal variables when choosing encoding strategy.</p>
  <p><strong>Example:</strong> Encoding education level as numeric values (High School=1, Bachelor=2, Master=3) for regression analysis.</p>
  <pre><code>factor_var <- factor(c("low", "medium", "high", "low"))
as.numeric(factor_var)  # Converts to 1,2,3</code></pre>

  <h3>4. Feature Engineering</h3>
  <p>Feature engineering creates new variables from existing data to improve model performance. Techniques include creating ratios, differences, aggregations, or domain-specific features. Good features capture underlying patterns and relationships that raw data alone may not reveal.</p>
  <p><strong>Example:</strong> In Iris dataset, the ratio of Sepal.Length to Sepal.Width can highlight elongated vs. short flowers, which may improve classification accuracy.</p>
  <pre><code>data$Sepal.Ratio <- data$Sepal.Length / iris$Sepal.Width
head(data$Sepal.Ratio)</code></pre>

  <h3>5. Outlier Detection</h3>
  <p>Outliers are extreme values that differ significantly from the rest of the data. They can distort statistical analysis and reduce model accuracy. Detection methods include boxplots, Z-scores, and IQR-based filtering. Once detected, outliers can be removed, transformed, or further investigated.</p>
  <p><strong>Example:</strong> In financial data, extreme transaction amounts may indicate fraud and should be flagged.</p>
  <pre><code>boxplot(iris$Sepal.Length, main = "Sepal Length Boxplot")
outliers <- boxplot.stats(iris$Sepal.Length)$out</code></pre>

  <h3>6. Practical Tips</h3>
  <ul>
    <li>Always explore data visually before preprocessing.</li>
    <li>Check distributions before imputation or scaling.</li>
    <li>Use domain knowledge to create meaningful features.</li>
    <li>Document all preprocessing steps to ensure reproducibility.</li>
    <li>Handle categorical variables carefully to avoid introducing ordinal assumptions if not present.</li>
  </ul>

</body>
</html>
